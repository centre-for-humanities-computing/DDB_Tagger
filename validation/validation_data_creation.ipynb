{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for Validation of DDB Tagger\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the pipeline to create a dataset for human validation of the DDB-Tagger. The aim is to sample sentences from a broad range of texts of the Danish Gigaword Corpus (DAGW), in which a target word is then tagged using the DDB Tagger. The sampled sentences, together with their target word and tags are stored in a dataframe. This dataframe can be used to allow human annotators to validate the tags of the DDB Tagger. This is done with the following pipeline:\n",
    " \n",
    "#### 0 Import Dependencies\n",
    "- Load Danish Language Model (SpaCy: `da_core_news_lg`) for splitting of sentences, tokens and POS-Tagging\n",
    "- Load DDB-Tagger\n",
    " \n",
    "#### 1 Defining Inputs for Sampling of Sentences and Target Words\n",
    "- <u>Categories</u>: Defining categories and sections of the DAGW corpus (related to dataset retrieved 20/02/2022)\n",
    "   - The following datasets were excluded:\n",
    "       - CONVERSATION: NAAT\n",
    "       - SOCIAL MEDIA: General Discussions, Parliament Elections\n",
    "       - WEB\n",
    "       - WIKI&BOOKS: Danish Literature, Gutenberg, WikiSource, Johannes V. Jensen, Religious Texts\n",
    "       - NEWS: DanAvis\n",
    "       - OTHER\n",
    "- <u>Subset Size</u>: Defines the number of characters which are read from a given sampled file. Defaults to 1000 characters.\n",
    "- <u>Context Size</u>: Defines the number of tokens to appear before a given target and after a given target in the same sentence. Defaults to 10 tokens.\n",
    "- <u>Target POS</u>: The target POS tag defines the POS tag that the target word in a given sentence should have. Defaults to `NOUN`.\n",
    " \n",
    "#### 2 Sampling Sentences and Target Words for Defined Categories\n",
    "The following steps are performed to retrieve target tokens and sentences and create the validation data (here described using the default inputs):<br>\n",
    "For each of the defined categories:\n",
    " \n",
    "- Create a list of files across all sections belonging to the given category\n",
    "- Until 20 targets/sentences have been found for the given category:\n",
    "- Sample a random file of the list of files and remove it from the list of files (to avoid double sampling)\n",
    "    - Read the first 1000 characters of the file\n",
    "    - Split the text into sentences, excluding those that contain line breaks\n",
    "    - For each sentence:\n",
    "        - Split the sentence into tokens, excluding space and punctuation\n",
    "        - If the number of tokens is large enough to contain a target word and 10 context tokens before and after the target (11):\n",
    "            - For each token, check if:\n",
    "                - Token is longer than 1 character\n",
    "                - Token occurs only once in the given sentence (to avoid confusion when tagging)\n",
    "                - Has the required POS tag (`NOUN`)\n",
    "                - Has 10 tokens before and 10 tokens after in the sentence\n",
    "                - Token does not occur in the validation data yet\n",
    "                - Sentence of the token does not occur in the validation data yet\n",
    "            - If the above requirements are fulfilled:\n",
    "                - Tag the sentence of the token using the DDB-Tagger\n",
    "                - Retrieve the tag of the target token in the sentence\n",
    "                - If the target token has 4+ tags:\n",
    "                    - Highlight the token in its sentence\n",
    "                    - Add information (token, sentence, tags) to the validation data\n",
    " \n",
    "#### 3 Processing Output\n",
    "- Processing output to create a dataset with all information (`validation_data_full.csv`) and a dataset for rating, with less information and a column for the ratings (`validation_data_rating.csv`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 Importing Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# danish language model\n",
    "import spacy\n",
    "nlp = spacy.load(\"da_core_news_lg\")\n",
    "\n",
    "# tagger\n",
    "sys.path.append(\"..\")\n",
    "from src.DDB_tagger import DDB_tagger\n",
    "Tagger = DDB_tagger(da_model=\"spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Defining Inputs for Sampling of Sentences and Target Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categories and sektions of DAGW to sample files from\n",
    "categories = {\"LEGAL\": [\"retsinformationdk\", \"skat\", \"retspraksis\"],\n",
    "              \"SOCIAL MEDIA\": [\"hest\"],\n",
    "              \"CONVERSATION\": [\"opensub\", \"ft\", \"ep\", \"spont\"],\n",
    "              \"WIKI&BOOKS\": [\"wiki\", \"wikibooks\"],\n",
    "              \"NEWS\": [\"tv2r\"]}\n",
    "\n",
    "# Define subset size (top n characters of a sampled file to read)\n",
    "subset_size = 1000\n",
    "\n",
    "# Define context size (n tokens before and after target in the same sentence)\n",
    "context_size = 10\n",
    "\n",
    "# Define POS tag of target words\n",
    "target_pos = \"NOUN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Sampling Sentences and Target Words for Defined Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------\n",
      "\n",
      "CATEGORY: LEGAL - found 83201 files to sample from.\n",
      "- Number of targets/sentences found: 20\n",
      "--------\n",
      "\n",
      "CATEGORY: SOCIAL MEDIA - found 14498 files to sample from.\n",
      "- Number of targets/sentences found: 20\n",
      "--------\n",
      "\n",
      "CATEGORY: CONVERSATION - found 38181 files to sample from.\n",
      "- Number of targets/sentences found: 20\n",
      "--------\n",
      "\n",
      "CATEGORY: WIKI&BOOKS - found 427497 files to sample from.\n",
      "- Number of targets/sentences found: 20\n",
      "--------\n",
      "\n",
      "CATEGORY: NEWS - found 49137 files to sample from.\n",
      "- Number of targets/sentences found: 20\r"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "list_of_dicts = []\n",
    "\n",
    "# Loop over categories and related sektions\n",
    "for category, sektions in categories.items():\n",
    "    \n",
    "    # --- GET FILES OF CATEGORY ---\n",
    "\n",
    "    category_files = []\n",
    "    # Loop over sektions\n",
    "    for sektion in sektions:\n",
    "        # Get path of directory for sektion\n",
    "        dir = f\"../../DAGW/sektioner/{sektion}/\"\n",
    "        # Define prefix of the files in the sektion\n",
    "        prefix = f\"{sektion}_\"\n",
    "        # Get all filepaths that start with the prefix from the directory\n",
    "        files = sorted([os.path.join(dir, file) for file in os.listdir(dir) if file.startswith(prefix)])\n",
    "        # Append sektion files to category files\n",
    "        category_files = category_files + files\n",
    "\n",
    "    # Print number of files which were found for category\n",
    "    print(f\"\\n--------\\n\")\n",
    "    print(f\"CATEGORY: {category} - found {len(category_files)} files to sample from.\")\n",
    "\n",
    "    # --- SAMPLE 20 SENTENCES FOR CATEGORY ---\n",
    "\n",
    "    category_sentences = 0\n",
    "    while category_sentences < 20:\n",
    "        \n",
    "        # Sample a random file from the category files\n",
    "        file = random.sample(category_files, 1)[0]\n",
    "        # Remove the file to avoid sampling it again\n",
    "        category_files.remove(file)\n",
    "        # Read the first n characters of the file\n",
    "        text = open(file, \"r\").read()[:subset_size]\n",
    "\n",
    "        # --- RETRIEVE APPROPRIATE SENTENCES AND TARGETS ---\n",
    "        \n",
    "        # Split sentences of text\n",
    "        sentences = [str(sent) for sent in nlp(text).sents if \"\\n\" not in str(sent)]\n",
    "        \n",
    "        # Loop over sentences\n",
    "        for sent in sentences:\n",
    "            \n",
    "            # Get only the tokens in the sentence (excluding punctuation and space)\n",
    "            tokens = [token.text for token in nlp(sent) if token.is_punct == False and token.is_space == False]\n",
    "            # Get tokens and additional information in the sentence\n",
    "            token_pos_idx = [(token.text, token.pos_, token.idx) for token in nlp(sent) if token.is_punct == False and token.is_space == False]\n",
    "            \n",
    "            # If the sentence is long enough to contain a target with sufficient context\n",
    "            if len(tokens) >= (context_size + 1 + context_size):\n",
    "                \n",
    "                # Loop over the tokens in the sentence\n",
    "                for idx, token_tuple in enumerate(token_pos_idx):\n",
    "                    \n",
    "                    # Save the info from the token\n",
    "                    target, pos, start_idx = token_tuple\n",
    "                    \n",
    "                    # If the token fulfils list of requirements: \n",
    "                        # Token is longer than a single charachter\n",
    "                    if (len(target) > 1 and                \n",
    "                        # Token only occurs once in the sentence\n",
    "                        tokens.count(target) == 1 and\n",
    "                        # Token has POS tag\n",
    "                        pos == target_pos and\n",
    "                        # Enough context before token\n",
    "                        idx > context_size and \n",
    "                        # Enough context after token\n",
    "                        idx < len(token_pos_idx) - context_size and \n",
    "                        # Target not in sampled sentences/targets\n",
    "                        target not in [d[\"TARGET\"] for d in list_of_dicts] and \n",
    "                        # Sentence not in sampled sentences/targets\n",
    "                        sent not in [d[\"SENT_ORIGINAL\"] for d in list_of_dicts]):\n",
    "\n",
    "                        # --- TAG TARGET TOKEN IF FULFILLING REQUIREMENTS ---\n",
    "\n",
    "                        # Tag the sentence\n",
    "                        sent_tagged = Tagger.tag_text(sent, only_top3_results=False, only_tagged_results=True)\n",
    "                        # Get only the tags of the target token\n",
    "                        target_tagged = sent_tagged[sent_tagged[\"TOKEN\"] == target].reset_index()\n",
    "\n",
    "                        # --- USE TARGET TOKEN IN VALIDATION DATA IF FULFILLING REQUIREMENTS ---\n",
    "\n",
    "                        # If the target token has 4 or more tags\n",
    "                        if target_tagged.at[0, \"DDB4+\"] != \"-\":\n",
    "\n",
    "                            # Highlight the token in the sentence for rating\n",
    "                            sent_highlight = sent[:start_idx] + \">>\" + sent[start_idx:start_idx+len(target)] + \"<<\" + sent[start_idx+len(target):]\n",
    "\n",
    "                            # Create a dictionary with all the info\n",
    "                            target_dict = {\"TARGET\": target,\n",
    "                                           \"SENT_ORIGINAL\": sent,\n",
    "                                           \"SENT_HIGHLIGHT\": sent_highlight,\n",
    "                                           \"CATEGORY\": category,\n",
    "                                           \"FILE\": file,\n",
    "                                           \"DDB1\": target_tagged.at[0, \"DDB1\"],\n",
    "                                           \"DDB2\": target_tagged.at[0, \"DDB2\"], \n",
    "                                           \"DDB3\": target_tagged.at[0, \"DDB3\"],\n",
    "                                           \"DDB4+\": target_tagged.at[0, \"DDB4+\"]}\n",
    "\n",
    "                            # Append the dictionary to all dictionaries\n",
    "                            list_of_dicts.append(target_dict)\n",
    "                            # Add count to number of category sentences\n",
    "                            category_sentences +=1\n",
    "                            # Print continuous count of number of sentences found for category\n",
    "                            print(f\"- Number of targets/sentences found: {category_sentences}\", end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Processing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESSING OUTPUT\n",
    "df_full = pd.DataFrame(list_of_dicts)\n",
    "df_rating = df_full.drop([\"SENT_ORIGINAL\", \"FILE\", \"CATEGORY\"], axis=1)\n",
    "df_rating[\"RATING\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    }
   ],
   "source": [
    "# CHECKING FOR DUPLICATES\n",
    "print(len(set(df_full[\"SENT_ORIGINAL\"].values)), len(set(df_full[\"SENT_HIGHLIGHT\"].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING DATAFRAMES\n",
    "df_full.to_csv(\"validation_data_full.csv\")\n",
    "df_rating.to_csv(\"validation_data_rating.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f583ef81dc38fab03fe8b79b65b3ec4feedd4b61d4894f7ca99dce04de81c351"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('venv_dbbtagger': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
